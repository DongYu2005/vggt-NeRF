import torch
import torch.nn as nn
from vggt.models.vggt import VGGT
import os
class VGGTFeatureExtractor(nn.Module):
    """
    Wrapper around the official VGGT model to extract dense feature maps.
    æ”¯æŒåŠ è½½é¢„è®­ç»ƒå‚æ•°å¹¶å†»ç»“æƒé‡ã€‚
    """
    def __init__(self, config):
        super().__init__()
        
        print("Initializing VGGT model...")

        # 1. å‡†å¤‡é…ç½®å‚æ•° (å…³æŽ‰ä¸ç”¨çš„ Head ä»¥èŠ‚çœæ˜¾å­˜)
        # è¿™äº›å‚æ•°ä¼šä¼ é€’ç»™æ¨¡åž‹åˆå§‹åŒ–
        model_kwargs = {
            "img_size": getattr(config, 'vggt_img_size', 518),
            "patch_size": getattr(config, 'vggt_patch_size', 14),
            "embed_dim": getattr(config, 'vggt_embed_dim', 1024),
            "enable_camera": False, 
            "enable_point": False, 
            "enable_depth": False, 
            "enable_track": False, 
            "enable_nlp": False
        }

        # 2. åŠ è½½æ¨¡åž‹
        # é€»è¾‘ï¼šå¦‚æžœ config é‡ŒæŒ‡å®šäº† vggt_ckpt è·¯å¾„ï¼Œå°±ç”¨æœ¬åœ°æ–‡ä»¶ï¼›å¦åˆ™ä»Ž Hugging Face ä¸‹è½½
        if hasattr(config, 'vggt_ckpt') and config.vggt_ckpt is not None:
            ckpt_path = config.vggt_ckpt
            print(f"ðŸš€ Checkpoint path provided: {ckpt_path}")
            if os.path.isdir(ckpt_path):
                    print(f"ðŸ“‚ Detected Folder. Loading via VGGT.from_pretrained()...")
                    self.model = VGGT.from_pretrained(ckpt_path, **model_kwargs)
                    print("âœ… HuggingFace weights loaded successfully from folder!")
            elif hasattr(config, 'vggt_ckpt') and config.vggt_ckpt is not None:
                print(f"Loading VGGT from LOCAL file: {config.vggt_ckpt}")
                # æœ¬åœ°æ¨¡å¼ï¼šæ‰‹åŠ¨åˆå§‹åŒ– + æ‰‹åŠ¨åŠ è½½æƒé‡ (å¤ç”¨ä½ ä¹‹å‰çš„é€»è¾‘)
                self.model = VGGT(**model_kwargs)
                try:
                    ckpt = torch.load(config.vggt_ckpt, map_location='cpu')
                    state_dict = ckpt['model'] if 'model' in ckpt else ckpt
                    # è¿‡æ»¤å¹¶åŠ è½½
                    feature_weights = {k: v for k, v in state_dict.items() if 'aggregator' in k}
                    self.model.load_state_dict(feature_weights, strict=False)
                    print("Local weights loaded successfully!")
                except Exception as e:
                    print(f"ERROR loading local weights: {e}")
            else:
                print(f"âŒ Error: Path does not exist: {ckpt_path}")
                # è¿™ç§æƒ…å†µä¸‹ä¸ºäº†é˜²æ­¢éšæœºè®­ç»ƒï¼Œå»ºè®®ç›´æŽ¥æŠ›é”™
                raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")
        
        else:
            print("Loading VGGT from Hugging Face (Automatic Download: facebook/VGGT-1B)...")
            # è‡ªåŠ¨æ¨¡å¼ï¼šfrom_pretrained ä¼šè‡ªåŠ¨ä¸‹è½½æƒé‡ã€åˆå§‹åŒ–ç»“æž„ã€å¹¶åº”ç”¨ model_kwargs
            # ä¸€æ­¥åˆ°ä½ï¼Œä¸éœ€è¦æ‰‹åŠ¨ torch.load
            local_model_path = "./checkpoints/VGGT-1B"
            # æ£€æŸ¥ä¸€ä¸‹è·¯å¾„å¯¹ä¸å¯¹ï¼Œé˜²æ­¢æ‰‹æ»‘å†™é”™
            if os.path.exists(local_model_path):
                print(f"Loading VGGT from local path: {local_model_path}")
                self.model = VGGT.from_pretrained(local_model_path, **model_kwargs)
            else:
                print(f"Error: Local path {local_model_path} not found!")
                # å¯ä»¥åœ¨è¿™é‡ŒæŠ›å‡ºå¼‚å¸¸æˆ–è€… fallback

        # 3. å†»ç»“å‚æ•° (Freezing)
        if getattr(config, 'freeze_vggt', True):
            print("Freezing VGGT weights (Feature Extraction Mode).")
            self.model.eval()
            for param in self.model.parameters():
                param.requires_grad = False
        else:
            print("VGGT weights are trainable (Fine-tuning Mode).")
            self.model.train()

    def forward(self, images):
        """
        Args:
            images: [B, S, 3, H, W] Input images (Normalized [0,1])
        Returns:
            feature_maps: [B, S, C, H_feat, W_feat]
        """
        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if images.dim() == 4: # [B, 3, H, W] -> [B, 1, 3, H, W]
            images = images.unsqueeze(1)
            
        B, S, C, H, W = images.shape
        
        # å¦‚æžœå†»ç»“äº†ï¼Œä½¿ç”¨ no_grad ä¸Šä¸‹æ–‡ä»¥èŠ‚çœæ˜¾å­˜
        # å¦‚æžœæ²¡å†»ç»“ï¼Œåˆ™æ­£å¸¸è®¡ç®—æ¢¯åº¦
        is_frozen = not next(self.model.parameters()).requires_grad
        
        with torch.set_grad_enabled(not is_frozen):
            # 1. è¿è¡Œ Aggregator
            output_list, patch_start_idx = self.model.aggregator(images)
            
            # 2. å–æœ€åŽä¸€å±‚ç‰¹å¾
            last_layer_tokens = output_list[-1]
            if last_layer_tokens.dim() == 4:
                # [B, S, Tokens, Dim] -> [B*S, Tokens, Dim]
                tokens_flat = last_layer_tokens.view(B * S, -1, last_layer_tokens.shape[-1])
            else:
                tokens_flat = last_layer_tokens
            # 3. å‰¥ç¦» Special Tokens
            patch_tokens = tokens_flat[:, patch_start_idx:, :]
            
            # 4. Reshape
            patch_size = self.model.aggregator.patch_size
            H_feat = H // patch_size
            W_feat = W // patch_size
            feat_dim = patch_tokens.shape[-1]
            
            feature_maps = patch_tokens.view(B * S, H_feat, W_feat, feat_dim)
            feature_maps = feature_maps.permute(0, 3, 1, 2)
            feature_maps = feature_maps.view(B, S, feat_dim, H_feat, W_feat)
            
        return feature_maps